{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 287/287 [09:43<00:00,  2.03s/files]\n",
      "Generating train split: 100%|██████████| 307373/307373 [09:03<00:00, 565.75 examples/s]\n",
      "Generating validation split: 100%|██████████| 7830/7830 [00:13<00:00, 598.29 examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-research-datasets/natural_questions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m topics \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/miniconda3/envs/rag_bench/lib/python3.10/site-packages/datasets/dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     76\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'question'"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"google-research-datasets/natural_questions\", \"default\")\n",
    "topics = ds[\"question\"].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adanato/miniconda3/envs/rag_bench/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 16:47:10 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 16:47:10,720\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 16:47:17 config.py:520] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 01-31 16:47:27 config.py:1328] Defaulting to use mp for distributed inference\n",
      "WARNING 01-31 16:47:27 arg_utils.py:1107] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 01-31 16:47:27 config.py:1483] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 01-31 16:47:27 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='meta-llama/Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 01-31 16:47:27 multiproc_worker_utils.py:298] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 01-31 16:47:27 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1642915)\u001b[0;0m INFO 01-31 16:47:27 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "INFO 01-31 16:47:28 cuda.py:225] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1642915)\u001b[0;0m INFO 01-31 16:47:28 cuda.py:225] Using Flash Attention backend.\n",
      "INFO 01-31 16:47:30 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1642915)\u001b[0;0m INFO 01-31 16:47:30 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 01-31 16:47:30 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1642915)\u001b[0;0m INFO 01-31 16:47:30 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 01-31 16:47:32 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/adanato/.cache/vllm/gpu_p2p_access_cache_for_2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1642915)\u001b[0;0m INFO 01-31 16:47:32 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/adanato/.cache/vllm/gpu_p2p_access_cache_for_2,3.json\n",
      "INFO 01-31 16:47:32 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_b730762c'), local_subscribe_port=54805, remote_subscribe_port=None)\n",
      "INFO 01-31 16:47:32 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1642915)\u001b[0;0m INFO 01-31 16:47:32 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-70B-Instruct...\n",
      "INFO 01-31 16:47:32 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1642915)\u001b[0;0m INFO 01-31 16:47:32 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    }
   ],
   "source": [
    "# 1. Install / Upgrade vLLM if needed\n",
    "# !pip install --upgrade vllm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "import pandas as pd\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "# 2. Specify the model name from the Hugging Face Hub\n",
    "model_name = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "\n",
    "# 3. Initialize the LLM engine\n",
    "llm = LLM(model=model_name, tensor_parallel_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "my_columns = [\"Column_A\", \"Column_B\", \"Column_C\", \"Column_D\", \"Column_E\", \"Column_F\", \"Column_G\"]\n",
    "\n",
    "# Create an empty DataFrame with those columns\n",
    "df = pd.DataFrame(columns=my_columns)\n",
    "prompts = [\n",
    "    \"what can I do to make friends\",\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "chats = []\n",
    "\n",
    "for prompt in prompts:\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    chats.append(chat)\n",
    "\n",
    "templated_chats = [tokenizer.apply_chat_template(chat, tokenize=False) for chat in chats]\n",
    "\n",
    "# 5. Define sampling (generation) parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7, \n",
    "    top_p=0.95, \n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# 6. Generate outputs\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# 7. Print generated text\n",
    "for i, output in enumerate(outputs):\n",
    "    prompt = prompts[i]\n",
    "    generated_text = output.outputs[0].text  # Take the first (or best) output\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Completion: {generated_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()  # Ensure chromedriver is in your PATH or specify its path\n",
    "\n",
    "try:\n",
    "    # Navigate to Google's homepage\n",
    "    driver.get(\"https://www.google.com\")\n",
    "\n",
    "    # Optionally, wait for the page to load completely\n",
    "    time.sleep(2)  # You can replace this with an explicit wait if preferred\n",
    "\n",
    "    # Locate the search input box using its 'name' attribute\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "\n",
    "    # Type your search query into the search box\n",
    "    query = \"Selenium WebDriver\"\n",
    "    search_box.send_keys(query)\n",
    "\n",
    "    # Submit the search form (this simulates pressing Enter)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait a few seconds for the search results to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Optionally, print the title of the current page to verify the results page loaded\n",
    "    print(\"Page title is:\", driver.title)\n",
    "\n",
    "finally:\n",
    "    # Close the browser window\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result verfication. Rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
